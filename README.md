
# Real-Time AI Voice Chat ğŸ¤ğŸ’¬ğŸ§ ğŸ”Š

**Have a natural, spoken conversation with an AI!**  

This project lets you chat with a Large Language Model (LLM) using just your voice, receiving spoken responses in near real-time. Think of it as your own digital conversation partner.

https://github.com/user-attachments/assets/16cc29a7-bec2-4dd0-a056-d213db798d8f

*(early preview - first reasonably stable version)*

## What's Under the Hood?

A sophisticated client-server system built for low-latency interaction:

1.  ğŸ™ï¸ **Capture:** Your voice is captured by your browser.
2.  â¡ï¸ **Stream:** Audio chunks are whisked away via WebSockets to a Python backend.
3.  âœï¸ **Transcribe:** `RealtimeSTT` rapidly converts your speech to text.
4.  ğŸ¤” **Think:** The text is sent to an LLM (like Ollama or OpenAI) for processing.
5.  ğŸ—£ï¸ **Synthesize:** The AI's text response is turned back into speech using `RealtimeTTS`.
6.  â¬…ï¸ **Return:** The generated audio is streamed back to your browser for playback.
7.  ğŸ”„ **Interrupt:** Jump in anytime! The system handles interruptions gracefully.

## Key Features âœ¨

*   **Fluid Conversation:** Speak and listen, just like a real chat.
*   **Real-Time Feedback:** See partial transcriptions and AI responses as they happen.
*   **Low Latency Focus:** Optimized architecture using audio chunk streaming.
*   **Smart Turn-Taking:** Dynamic silence detection (`turndetect.py`) adapts to the conversation pace.
*   **Flexible AI Brains:** Pluggable LLM backends (Ollama default, OpenAI support via `llm_module.py`).
*   **Customizable Voices:** Choose from different Text-to-Speech engines (Kokoro, Coqui, Orpheus via `audio_module.py`).
*   **Web Interface:** Clean and simple UI using Vanilla JS and the Web Audio API.
*   **Dockerized Deployment:** Recommended setup using Docker Compose for easier dependency management.

## Technology Stack ğŸ› ï¸

*   **Backend:** Python < 3.13, FastAPI
*   **Frontend:** HTML, CSS, JavaScript (Vanilla JS, Web Audio API, AudioWorklets)
*   **Communication:** WebSockets
*   **Containerization:** Docker, Docker Compose
*   **Core AI/ML Libraries:**
    *   `RealtimeSTT` (Speech-to-Text)
    *   `RealtimeTTS` (Text-to-Speech)
    *   `transformers` (Turn detection, Tokenization)
    *   `torch` / `torchaudio` (ML Framework)
    *   `ollama` / `openai` (LLM Clients)
*   **Audio Processing:** `numpy`, `scipy`

## Before You Dive In: Prerequisites ğŸŠâ€â™€ï¸

This project leverages powerful AI models, which have some requirements:

*   **Operating System:**
    *   **Docker:** Linux is recommended for the best GPU integration with Docker.
    *   **Manual:** The provided script (`install.bat`) is for Windows. Manual steps are possible on Linux/macOS but may require more troubleshooting (especially for DeepSpeed).
*   **ğŸ Python:** 3.9 or higher (if setting up manually).
*   **ğŸš€ GPU:** **A powerful CUDA-enabled NVIDIA GPU is *highly recommended***, especially for faster STT (Whisper) and TTS (Coqui). Performance on CPU-only or weaker GPUs will be significantly slower.
    *   The setup assumes **CUDA 12.1**. Adjust PyTorch installation if you have a different CUDA version.
    *   **Docker (Linux):** Requires [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html).
*   **ğŸ³ Docker (Optional but Recommended):** Docker Engine and Docker Compose v2+ for the containerized setup.
*   **ğŸ§  Ollama (Optional):** If using the Ollama backend *without* Docker, install it separately and pull your desired models. The Docker setup includes an Ollama service.
*   **ğŸ”‘ OpenAI API Key (Optional):** If using the OpenAI backend, set the `OPENAI_API_KEY` environment variable (e.g., in a `.env` file or passed to Docker).

---

## Getting Started: Installation & Setup âš™ï¸

**Clone the repository first:**

```bash
# runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04
#On-Demand - Secure Cloud

git clone https://github.com/zwong91/RealtimeVoiceChat.git
cd RealtimeVoiceChat

# System dependencies (Ubuntu/Debian)
apt update
apt-get -qq -y install espeak-ng > /dev/null 2>&1
apt install curl lshw ffmpeg libopenblas-dev vim git-lfs \
    build-essential cmake libasound-dev portaudio19-dev \
    libportaudio2 -y

# Install uv
curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env


# Create and activate Python 3.10 virtual environment named 'agent'
uv venv --python=python3.10 agent
source agent/bin/activate

curl -sS https://bootstrap.pypa.io/get-pip.py -o get-pip.py
python get-pip.py

# Install dependencies using uv
uv pip install -r requirements.txt

# Install ollama https://github.com/ollama/ollama/releases
(curl -fsSL https://ollama.com/install.sh | sh && ollama serve > ollama.log 2>&1) &

ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M

## zh-hans
mkdir -p /root/stanza_resources/zh-hans/

pip install -U "huggingface_hub[cli]"
huggingface-cli download stanfordnlp/stanza-zh-hans --local-dir stanza-zh-hans

mv stanza-zh-hans/models/*  /root/stanza_resources/zh-hans/

huggingface-cli login <hf_token> 

git lfs install
git clone https://huggingface.co/stanfordnlp/stanza-zh-hans
# ```python
# import stanza
# stanza.download(lang="zh-hans", processors={"pos": "gsdsimp_charlm"})
# ```

## å®‰è£… cuBLAS å’Œ cuDNN
apt install -y cuda-toolkit-12-4

wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
dpkg -i cuda-keyring_1.1-1_all.deb
apt-get update
apt-get -y install cudnn

apt-get install libcudnn8 && apt-get install libcudnn8-dev

```

Now, choose your adventure:

<details>
<summary><strong>ğŸš€ Option A: Docker Installation (Recommended for Linux/GPU)</strong></summary>

This is the most straightforward method, bundling the application, dependencies, and even Ollama into manageable containers.

1.  **Build the Docker images:**
    *(This takes time! It downloads base images, installs Python/ML dependencies, and pre-downloads the default STT model.)*
    ```bash
    docker compose build
    ```
    *(If you want to customize models/settings in `code/*.py`, do it **before** this step!)*

2.  **Start the services (App & Ollama):**
    *(Runs containers in the background. GPU access is configured in `docker-compose.yml`.)*
    ```bash
    docker compose up -d
    ```
    Give them a minute to initialize.

3.  **(Crucial!) Pull your desired Ollama Model:**
    *(This is done *after* startup to keep the main app image smaller and allow model changes without rebuilding. Execute this command to pull the default model into the running Ollama container.)*
    ```bash
    # Pull the default model (adjust if you configured a different one in server.py)
    docker compose exec ollama ollama pull hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M

    # (Optional) Verify the model is available
    docker compose exec ollama ollama list
    ```

4.  **Stopping the Services:**
    ```bash
    docker compose down
    ```

5.  **Restarting:**
    ```bash
    docker compose up -d
    ```

6.  **Viewing Logs / Debugging:**
    *   Follow app logs: `docker compose logs -f app`
    *   Follow Ollama logs: `docker compose logs -f ollama`
    *   Save logs to file: `docker compose logs app > app_logs.txt`

</details>

<details>
<summary><strong>ğŸ› ï¸ Option B: Manual Installation (Windows Script / venv)</strong></summary>

This method requires managing the Python environment yourself. It offers more direct control but can be trickier, especially regarding ML dependencies.

**B1) Using the Windows Install Script:**

1.  Ensure you meet the prerequisites (Python, potentially CUDA drivers).
2.  Run the script. It attempts to create a venv, install PyTorch for CUDA 12.1, a compatible DeepSpeed wheel, and other requirements.
    ```batch
    install.bat
    ```
    *(This opens a new command prompt within the activated virtual environment.)*
    Proceed to the **"Running the Application"** section.

**B2) Manual Steps (Linux/macOS/Windows):**

1.  **Create & Activate Virtual Environment:**
    ```bash
    python -m venv venv
    # Linux/macOS:
    source venv/bin/activate
    # Windows:
    .\venv\Scripts\activate
    ```

2.  **Upgrade Pip:**
    ```bash
    python -m pip install --upgrade pip
    ```

3.  **Navigate to Code Directory:**
    ```bash
    cd code
    ```

4.  **Install PyTorch (Crucial Step - Match Your Hardware!):**
    *   **With NVIDIA GPU (CUDA 12.1 Example):**
        ```bash
        # Verify your CUDA version! Adjust 'cu121' and the URL if needed.
        pip install torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision --index-url https://download.pytorch.org/whl/cu121
        ```
    *   **CPU Only (Expect Slow Performance):**
        ```bash
        # pip install torch torchaudio torchvision
        ```
    *   *Find other PyTorch versions:* [https://pytorch.org/get-started/previous-versions/](https://pytorch.org/get-started/previous-versions/)

5.  **Install Other Requirements:**
    ```bash
    pip install -r requirements.txt
    ```
    *   **Note on DeepSpeed:** The `requirements.txt` may include DeepSpeed. Installation can be complex, especially on Windows. The `install.bat` tries a precompiled wheel. If manual installation fails, you might need to build it from source or consult resources like [deepspeedpatcher](https://github.com/erew123/deepspeedpatcher) (use at your own risk). Coqui TTS performance benefits most from DeepSpeed.

</details>

---

## Running the Application â–¶ï¸

**If using Docker:**
Your application is already running via `docker compose up -d`! Check logs using `docker compose logs -f src`.

**If using Manual/Script Installation:**

1.  **Activate your virtual environment** (if not already active):
    ```bash
    # Linux/macOS: source ./agent/bin/activate
    # Windows: .\agent\Scripts\activate
    ```
2.  **Navigate to the `src` directory** (if not already there):
    ```bash
    cd src
    ```
3.  **Start the FastAPI server:**
    ```bash
    python server.py
    ```

**Accessing the Client (Both Methods):**

1.  Open your web browser to `http://localhost:8000` (or your server's IP if running remotely/in Docker on another machine).
2.  **Grant microphone permissions** when prompted.
3.  Click **"Start"** to begin chatting! Use "Stop" to end and "Reset" to clear the conversation.

---

## Configuration Deep Dive ğŸ”§

Want to tweak the AI's voice, brain, or how it listens? Modify the Python files in the `code/` directory.

**âš ï¸ Important Docker Note:** If using Docker, make any configuration changes *before* running `docker compose build` to ensure they are included in the image.

*   **TTS Engine & Voice (`server.py`, `audio_module.py`):**
    *   Change `START_ENGINE` in `server.py` to `"coqui"`, `"kokoro"`, or `"orpheus"`.
    *   Adjust engine-specific settings (e.g., voice model path for Coqui, speaker ID for Orpheus, speed) within `AudioProcessor.__init__` in `audio_module.py`.
*   **LLM Backend & Model (`server.py`, `llm_module.py`):**
    *   Set `LLM_START_PROVIDER` (`"ollama"` or `"openai"`) and `LLM_START_MODEL` (e.g., `"hf.co/..."` for Ollama, model name for OpenAI) in `server.py`. Remember to pull the Ollama model if using Docker (see Installation Step A3).
    *   Customize the AI's personality by editing `system_prompt.txt`.
*   **STT Settings (`transcribe.py`):**
    *   Modify `DEFAULT_RECORDER_CONFIG` to change the Whisper model (`model`), language (`language`), silence thresholds (`silence_limit_seconds`), etc. The default `deepdml/faster-whisper-large-v3-turbo-ct2` model is pre-downloaded during the Docker build.
*   **Turn Detection Sensitivity (`turndetect.py`):**
    *   Adjust pause duration constants within the `TurnDetector.update_settings` method.
*   **SSL/HTTPS (`server.py`):**
    *   Set `USE_SSL = True` and provide paths to your certificate (`SSL_CERT_PATH`) and key (`SSL_KEY_PATH`) files.
    *   **Docker Users:** You'll need to adjust `docker-compose.yml` to map the SSL port (e.g., 443) and potentially mount your certificate files as volumes.
    <details>
    <summary><strong>Generating Local SSL Certificates (Windows Example w/ mkcert)</strong></summary>

    1.  Install Chocolatey package manager if you haven't already.
    2.  Install mkcert: `choco install mkcert`
    3.  Run Command Prompt *as Administrator*.
    4.  Install a local Certificate Authority: `mkcert -install`
    5.  Generate certs (replace `your.local.ip`): `mkcert localhost 127.0.0.1 ::1 your.local.ip`
        *   This creates `.pem` files (e.g., `localhost+3.pem` and `localhost+3-key.pem`) in the current directory. Update `SSL_CERT_PATH` and `SSL_KEY_PATH` in `server.py` accordingly. Remember to potentially mount these into your Docker container.
    </details>

---

## Q & A
/ws åœ¨å¤šä¸ª websocket è¿æ¥æœŸé—´ï¼ŒéŸ³é¢‘ç‰‡æ®µç­‰å†…å®¹å¯èƒ½ä¼šè¢«è¦†ç›–ã€‚åœ¨å‚ç›´æ‰©å±•è¿™æ ·çš„é¡¹ç›®æ—¶ï¼Œæœ‰å¾ˆå¤šç“¶é¢ˆéœ€è¦å¤„ç†ã€‚
æœ€ç³Ÿç³•çš„æƒ…å†µæ˜¯ä¸¤ä¸ªæˆ–æ›´å¤šç”¨æˆ·å‡ ä¹åŒæ—¶å®Œæˆä»–ä»¬çš„å¥å­ã€‚è¿™æ—¶ä¼šå‡ºç°å³°å€¼ GPU è´Ÿè½½ï¼Œå¹¶ä¸”åœ¨è¿™ç§æƒ…å†µä¸‹æ— æ³•ä¿è¯ä½å»¶è¿Ÿã€‚
æ­¤å¤–ï¼ŒRealtimeSTT å’Œ RealtimeTTS éƒ½æ— æ³•å¤„ç†å¤šä¸ªå¹¶è¡Œè¯·æ±‚ã€‚å› æ­¤ï¼Œç›®å‰æ¯ä¸ªå®ä¾‹å®é™…ä¸Šåªèƒ½æ”¯æŒä¸€ä¸ªç”¨æˆ·ï¼Œå¦åˆ™ä½ éœ€è¦è¿›è¡Œæ¨ªå‘æ‰©å±•ã€‚

è¿›è¡Œå‚ç›´æ‰©å±•åˆ™åœ¨æ¯ä¸ªæ–°çš„ websocket è¿æ¥ä¸Šåˆå§‹åŒ– AudioInputProcessor ä»¥åŠå…¶ä»–ç±»ï¼Œ
RealtimeSTT å’Œ RealtimeTTS éƒ½æ— æ³•å¤„ç†å¹¶è¡Œè¯·æ±‚ï¼Œè€Œè¿™åœ¨å¤šä¸ªç”¨æˆ·åœºæ™¯ä¸­æ˜¯å¿…éœ€çš„ã€‚
å»¶è¿Ÿä¸‹é™å°†éå¸¸æ˜æ˜¾ï¼Œæ•´ä½“æ€§èƒ½å°†å¤§å¤§å—æŸã€‚å³ä½¿åªæœ‰ä¸€ä¸ªç”¨æˆ·ï¼ŒRealtimeVoiceChat ä¹Ÿç¡®å®éœ€è¦ä¸€å—ç›¸å½“å¼ºå¤§çš„ GPU æ‰èƒ½é¡ºç•…è¿è¡Œã€‚å®æ—¶è½¬å½•çš„å‚ç›´æ‰©å±•æ¶‰åŠçš„å†…å®¹éå¸¸å¤æ‚ï¼Œé¦–å…ˆï¼Œä½ é¢ä¸´çš„é—®é¢˜æ˜¯æ¥è‡ªä¸åŒå®¢æˆ·ç«¯çš„éŸ³é¢‘å—éœ€è¦è¿›è¡Œè¯­éŸ³æ´»åŠ¨æ£€æµ‹ï¼ˆVADï¼‰ã€‚å³ä½¿ä½ è®¾æ³•é€šè¿‡æ‰¹å¤„ç†å®Œç¾åœ°å¹¶è¡ŒåŒ–è½¬å½•ï¼Œä½ ä»ç„¶ä¼šåœ¨ SileroVAD æˆ– webrtcVAD ä¸Šé‡åˆ°ç“¶é¢ˆï¼Œï¼Œå› ä¸ºè¿™ä¸¤è€…éƒ½ä¸æ”¯æŒæ‰¹å¤„ç†ã€‚ä¸¤è€…éƒ½ä¼šå¼•å…¥å»¶è¿Ÿã€‚å› æ­¤ï¼Œå‡è®¾ 10 ä¸ªéŸ³é¢‘å—å¹¶è¡Œåˆ°è¾¾ï¼Œä½ å¤„ç† 10 å€çš„ SileroVADï¼Œå¢åŠ çš„å»¶è¿Ÿå°†ä½¿å®æ—¶å¤„ç†å˜å¾—ä¸å¯èƒ½ã€‚
æ­¤å¤–ï¼Œæ‰¹å¤„ç†å¯èƒ½åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚ï¼Œä½†å®¢æˆ·ç«¯çš„è½¬å½•è¯·æ±‚å¹¶ä¸æ˜¯å®Œç¾åŒæ­¥åˆ°è¾¾çš„ã€‚ä½ å¿…é¡»æ‰‹åŠ¨å»¶è¿Ÿå®ƒä»¬ï¼Œç­‰å¾…åœ¨ç‰¹å®šæ—¶é—´çª—å£å†…åˆ°è¾¾è¶³å¤Ÿçš„è¯·æ±‚ä»¥å½¢æˆä¸€ä¸ªæ‰¹æ¬¡ã€‚ä»…è¿™ä¸€ç‚¹å°±å¢åŠ äº†å»¶è¿Ÿï¼Œç ´åäº†å®¢æˆ·çš„å®æ—¶ä½“éªŒã€‚
åŸºäº faster_whisperï¼Œåªèƒ½é€šè¿‡ä¸¤ç§æ–¹å¼å¤„ç†å¹¶è¡Œè¯­éŸ³è½¬æ–‡æœ¬ï¼šè¦ä¹ˆåˆ©ç”¨å¤šä¸ª GPUï¼Œè¦ä¹ˆæ‰¹å¤„ç†ä¸€ä¸ªè¾ƒå¤§çš„éŸ³é¢‘è¾“å…¥æ–‡ä»¶,
è¿™ä¸¤ç§é€‰é¡¹éƒ½ä¸æ”¯æŒå¯¹å¤šä¸ªä¼ å…¥è¯·æ±‚çš„çœŸæ­£å¹¶å‘è½¬å½•ã€‚

TTS Concurrency æ­»é”ã€GPU èµ„æºç«äº‰å’Œé«˜å»¶è¿Ÿçš„é—®é¢˜ã€‚ä¸èƒ½å¯¹å¤šä¸ªå¹¶å‘è¯·æ±‚ä½¿ç”¨ç›¸åŒçš„å¼•æ“ï¼Œå› ä¸ºå®ƒä»¬æœ€ç»ˆéƒ½ä¼šå†™å…¥åŒä¸€ä¸ªéŸ³é¢‘é˜Ÿåˆ—ã€‚è¦å¤„ç†çœŸæ­£çš„å¹¶å‘åˆæˆï¼Œä½ éœ€è¦å¤šä¸ªå¼•æ“å®ä¾‹ã€‚
å¯¹äº CoquiTTS å’Œå¤§å¤šæ•°å…¶ä»–æœ¬åœ°å¼•æ“æ¥è¯´ï¼Œè¿™å¹¶æ²¡æœ‰å¤ªå¤§æ„ä¹‰ã€‚æ‰€æœ‰è¿™äº›å¼•æ“éƒ½ä½¿ç”¨ 100%çš„ GPUï¼Œå¦‚æœä½ å¹¶è¡ŒåŒ–è¯·æ±‚ï¼Œåªä¼šä½¿å®ƒä»¬å˜æ…¢ã€‚
ä¸åŒçš„ TTS æ¨¡å‹ï¼š
1. Token-by-Token Streaming Models
åƒ Coqui XTTS è¿™æ ·çš„æ¨¡å‹å¯ä»¥é€ä¸ªéŸ³é¢‘ä»¤ç‰Œåœ°æµå¼ä¼ è¾“éŸ³é¢‘ã€‚è¿™è®©ä½ å¯ä»¥åœ¨éœ€è¦æ—¶å‡æ…¢åˆæˆé€Ÿåº¦ã€‚
ä¾‹å¦‚ï¼šå¦‚æœä½ çš„å®æ—¶å› å­ä½äº 0.5ï¼Œä½ å¯ä»¥é€šè¿‡å°†æ¯ä¸ªåˆæˆçš„é€Ÿåº¦å‡æ…¢åˆ°ç•¥ä½äº 1 æ¥åŒæ—¶è¿è¡Œä¸¤ä¸ªåˆæˆã€‚
è¿™æ ·ï¼Œå•ä¸ªåˆæˆä¸ä¼šé˜»å¡ GPUï¼Œä½ å¯ä»¥åŒæ—¶ä¸ºä¸¤ä¸ªç”¨æˆ·æä¾›æœåŠ¡ï¼Œè€Œä¸ä¼šå¤§å¹…å»¶è¿Ÿç¬¬ä¸€ä¸ªéŸ³é¢‘ä»¤ç‰Œã€‚Coqui åœ¨å¿«é€Ÿç³»ç»Ÿä¸Šå¯ä»¥è¾¾åˆ° <0.2 çš„å®æ—¶å› å­ -> 5 ä¸ªç”¨æˆ·å¹¶è¡Œã€‚
åœ¨ rtf ä¸º 0.2 çš„æƒ…å†µä¸‹ä¼šå¯åŠ¨ 5 ä¸ªå¹¶è¡Œåˆæˆï¼Œä½†è¿™æ—¶ GPU è´Ÿè½½å·²æ»¡ã€‚ç¬¬å…­ä¸ªç”¨æˆ·å¿…é¡»ç­‰åˆ°å…¶ä»–åˆæˆä¸­çš„ä¸€ä¸ªå®Œæˆï¼Œå¯èƒ½éœ€è¦å‡ ç§’é’Ÿã€‚
åœ¨RTF <0.03ç”Ÿæˆé€Ÿåº¦è¿˜ä¸é”™, æ— è®ºæ˜¯åœ¨ 4090ã€A40ã€A100 è¿˜æ˜¯ H100 ä¸Šæ¨ç†æ²¡å•¥åŒºåˆ«ï¼Œ
å®ç°ä½äº 200 æ¯«ç§’çš„é¦–ä¸ªéŸ³é¢‘åŒ…å»¶è¿Ÿï¼Œå‡å°‘æ‰©æ•£æ­¥éª¤å’Œç¼©çŸ­åˆå§‹éŸ³é¢‘ï¼ˆæ¯”å¦‚ 2-3 ç§’ï¼‰å°†è¾“å…¥æ–‡æœ¬åˆ†æˆä¸¤éƒ¨åˆ†ï¼ˆæˆ–æ›´å¤šï¼‰å¹¶é€ä¸ªåˆæˆã€‚
ç¬¬ä¸€ä¸ªâ€œ,â€æˆ–â€œ.â€ç­‰å¤„åˆ‡æ–‡æœ¬, éƒ¨åˆ†æ’­æ”¾çš„åŒæ—¶ï¼Œä½ å¯ä»¥è·å¾—è¶³å¤Ÿçš„æ—¶é—´æ¥åˆæˆå…¶ä½™éƒ¨åˆ†


2. Diffusion Models
è€Œæ‰©æ•£æ¨¡å‹å¦‚ StyleTTS2ï¼ˆæˆ– Kokoroï¼‰åˆ™èƒ½å¤Ÿéå¸¸å¿«é€Ÿåœ°ç”Ÿæˆå®Œæ•´çš„åˆæˆï¼Œ<100msï¼Œåœ¨å¿«é€Ÿç³»ç»Ÿä¸Šä¸º 50msã€‚å› æ­¤ï¼Œè¿™ç§é€Ÿåº¦ä½¿å®ƒä»¬é€‚åˆå®æ—¶åº”ç”¨ï¼Œå³ä½¿å®ƒä»¬æ˜¯é¡ºåºå¤„ç†è¯·æ±‚ï¼Œå› ä¸ºæ¯ä¸ªè¯·æ±‚éƒ½ä¼šå®Œå…¨é˜»å¡ GPUã€‚
6 ä¸ªä¼ å…¥è¯·æ±‚, ä»¥æ¯ä¸ªè¯·æ±‚ 50 æ¯«ç§’çš„é€Ÿåº¦å¼€å§‹å¤„ç†ï¼Œç¬¬å…­ä¸ªç”¨æˆ·å¤§çº¦éœ€è¦ç­‰å¾… 250 æ¯«ç§’ã€‚

3. LLM-Based TTS Systems
åŸºäº LLM æ¨ç†çš„æ–°å…´ TTS ç³»ç»Ÿï¼Œæ¯”å¦‚ Orpheusï¼Œå®ƒå¯ä»¥åœ¨ä»»ä½• LLM æä¾›å•†ä¸Šè¿è¡Œã€‚è¿™äº›ç³»ç»Ÿè®©ä½ å¯ä»¥ä½¿ç”¨ vLLM è¿›è¡ŒæœåŠ¡å¹¶å‘æ‰¹é‡å¤„ç†ï¼Œ
åœ¨é«˜è´Ÿè½½ç¯å¢ƒä¸­ï¼Œè¿™ç§æ–¹æ³•å¯èƒ½éå¸¸æœ‰å‰æ™¯ï¼Œä½ å¯ä»¥åœ¨æ²¡æœ‰æ˜¾è‘—å»¶è¿Ÿçš„æƒ…å†µä¸‹æ‰¹é‡å¤„ç†å¤šä¸ªä¼ å…¥çš„ TTS è¯·æ±‚ã€‚


**æ¨èé‡‡ç”¨æ°´å¹³æ‰©å±•çš„æ–¹æ³•ï¼Œä½¿ç”¨å¤šä¸ªå®ä¾‹ï¼Œæ¯ä¸ªå®ä¾‹åœ¨ä¸€ä¸ªç‹¬ç«‹çš„ GPU ä¸Šã€‚**

24GB æ˜¾å­˜ï¼ˆRTX 3090/4090) è¿è¡Œå½“å‰æ¨¡å‹çš„é¦–æ¬¡ä»¤ç‰Œæ—¶é—´TTFT ä¸º 0.0563 ç§’ï¼Œæ¨ç†é€Ÿåº¦ä¸º 52.85 token/ç§’ã€‚
16GB çš„è¯åˆ™ é¦–æ¬¡ä»¤ç‰Œæ—¶é—´ä½äº 100 æ¯«ç§’ï¼Œé€Ÿåº¦è¶…è¿‡ 30 ä¸ªä»¤ç‰Œæ¯ç§’ã€‚Holy fuck LLM: 139.02ms, TTS: 59.90ms

"Unable to load any of {libcudnn_ops.so.9.1.0, libcudnn_ops.so.9.1, libcudnn_ops.so.9, libcudnn_ops.so}" error
pip install "ctranslate2<4.5.0"

* VAD: Webrtcvad (first fast check) followed by SileroVAD (high compute verification)
* Transcription: deepdml/faster-whisper-large-v3-turbo-ct2 whisper (CTranslate2) 300ms
* Turn Detection: livekit/turn-detector (qwen2.5:0.5b base model)
sentence binary classification model  20-50ms
* LLM: hf.co/bartowski/huihui-ai_Mistral-Small-24B-Instruct-2501-abliterated-GGUF:Q4_K_M (easily switchable) 140ms
* TTS: Coqui XTTSv2, switchable to Kokoro or Orpheus (this one is slower) 80ms

https://medium.com/@lonligrin/improving-voice-ai-with-a-sentence-completeness-classifier-2da6e950538a
åŸºäº Silero-VAD çš„è½¬å¼¯æ£€æµ‹ä»…åœ¨ç”¨æˆ·åœæ­¢è¯´è¯åä½¿ç”¨å›ºå®šçš„æ²‰é»˜æ—¶é—´ï¼Œç„¶åå†³å®šâ€œè½¬å¼¯ç»“æŸâ€ã€‚è¿™éå¸¸å¹¼ç¨šã€‚
æˆ‘ä½¿ç”¨å®æ—¶è½¬å½•ï¼Œç„¶åç»“åˆæç¤º whisper åœ¨æœªå®Œæˆçš„å¥å­ä¸Šæ·»åŠ çœç•¥å·ï¼Œä»¥åŠä¸Šè¿°æ¨¡å‹æ¥åˆ¤æ–­å¥å­æ˜¯å¦å®Œæ•´ã€‚
è¿™è¿œéå®Œç¾ï¼Œä½†ç›¸è¾ƒäºä½¿ç”¨å•çº¯çš„æ²‰é»˜æ¥è¯´æ˜¯ä¸€ä¸ªå®è´¨æ€§çš„æ”¹è¿›ã€‚

MPV æµ è€Œä¸æ˜¯ PCM
æ— æ³•åŠ è½½ä»»ä½• {libcudnn_ops.so.9.1.0, libcudnn_ops.so.9.1, libcudnn_ops.so.9, libcudnn_ops.so}ï¼š

```bash
#å®‰è£… cuBLAS å’Œ cuDNN
apt install -y cuda-toolkit-12-4
https://developer.nvidia.com/cudnn-downloads
```

Could not load library libcudnn_ops_infer.so.8. Error: libcudnn_ops_infer.so.8: cannot open shared object file: No such file or directory
18:18.74 root       ERRO conn.recv() error:  occurred in the synthesize worker thread of Coqui engine.

```bash
apt install libcudnn8 libcudnn8-dev
```

<https://github.com/KoljaB/RealtimeVoiceChat/blob/main/code/audio_module.py#L108>

åº”ç”¨å®¹å™¨ä¸­åˆ›å»ºä¸€ä¸ªå­æ–‡ä»¶å¤¹ï¼š./models/some_folder_name
 å°†æ‚¨æ‰€éœ€çš„è¯­éŸ³æ¨¡å‹æ–‡ä»¶å¤åˆ¶åˆ°è¯¥æ–‡ä»¶å¤¹ä¸­ï¼šconfig.jsonã€model.pthã€vocab.json å’Œ
speakers_xtts.pthï¼ˆå¯ä»¥ä» Lasinya å¤åˆ¶ speakers_xtts.pthï¼Œå®ƒå¯¹æ¯ä¸ªè¯­éŸ³éƒ½æ˜¯ç›¸åŒçš„ã€‚ç„¶åå°† audio_module.py ä¸­çš„ specific_model="Lasinya" è¡Œæ›´æ”¹ä¸º specific_model="some_folder_name"ã€‚

Lasinya voice ä½¿ç”¨è‡ªåˆ›çš„åˆæˆæ•°æ®é›†åˆ¶ä½œçš„ XTTS 2.0.2 å¾®è°ƒç‰ˆæœ¬ã€‚ä½¿ç”¨äº† <https://github.com/daswer123/xtts-finetune-webui> è¿›è¡Œè®­ç»ƒã€‚

dataset zh  keywords
ivanzhu109/zh-taiwan
JacobLinCool/jacob-common-voice-19-zh-TW-curated


<https://huggingface.co/sandy1990418/xtts-v2-chinese>
<https://github.com/idiap/coqui-ai-TTS/blob/dev/TTS/demos/xtts_ft_demo/XTTS_finetune_colab.ipynb>

turn modelè³ªé‡å’Œé€Ÿåº¦çš„æœ€ä½³å¹³è¡¡ã€‚

```python
import os
import random
import torch
import numpy as np
import pandas as pd

from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    roc_auc_score,
    matthews_corrcoef,
    balanced_accuracy_score,
)

from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments,
    EarlyStoppingCallback,
)

from torch.utils.tensorboard import SummaryWriter

# -----------------------------
# Configuration
# -----------------------------
data_dir = "dataset"
complete_file = "dataset/filtered_complete_sentences.txt"
incomplete_file = "dataset/filtered_incomplete_sentences.txt"
# complete_file = os.path.join(data_dir, "complete_sentences.txt")
# incomplete_file = os.path.join(data_dir, "incomplete_sentences.txt")

model_name = "distilbert-base-uncased"
output_dir = "./better-distil-finetuned_model"

num_train_epochs = 30
batch_size = 512
learning_rate = 2e-5

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)

# -----------------------------
# Load and prepare the dataset
# -----------------------------
def load_sentences(file_path, label):
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.read().strip().split("\n")
    return [(line.strip().lower(), label) for line in lines if line.strip()]

complete_data = load_sentences(complete_file, 1)    # label 1 for complete
incomplete_data = load_sentences(incomplete_file, 0) # label 0 for incomplete

all_data = complete_data + incomplete_data
random.shuffle(all_data)

df = pd.DataFrame(all_data, columns=["text", "label"])

# Verify class counts
print(f"Complete sentences: {df['label'].sum()}")
print(f"Incomplete sentences: {len(df) - df['label'].sum()}")

train_df, val_df = train_test_split(
    df, 
    test_size=0.1, 
    random_state=42, 
    stratify=df["label"]
)

tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)

def tokenize_fn(examples):
    return tokenizer(
        examples["text"], 
        truncation=True, 
        padding="max_length", 
        max_length=128
    )

train_dataset_hf = Dataset.from_pandas(train_df.reset_index(drop=True))
val_dataset_hf = Dataset.from_pandas(val_df.reset_index(drop=True))

train_dataset_hf = train_dataset_hf.map(tokenize_fn, batched=True)
val_dataset_hf = val_dataset_hf.map(tokenize_fn, batched=True)

train_dataset_hf.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
val_dataset_hf.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

# -----------------------------
# Model and Training Arguments
# -----------------------------
model = DistilBertForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2
)

training_args = TrainingArguments(
    output_dir=output_dir,
    overwrite_output_dir=True,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_train_epochs,
    learning_rate=learning_rate,
    logging_dir="./logs",
    logging_steps=50,
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",  # Use accuracy instead of the composite metric
    greater_is_better=True,
    save_total_limit=3,
    weight_decay=0.01,
    fp16=True,
    report_to="none"
)

# -----------------------------
# Composite Metric Computation
# -----------------------------
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=1)

    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="weighted")

    # For ROC-AUC, we need the probability of the positive class
    # logits[:,1] gives the predicted logits for the positive class
    roc_auc = roc_auc_score(labels, logits[:,1])  
    mcc = matthews_corrcoef(labels, predictions)
    balanced_acc = balanced_accuracy_score(labels, predictions)

    # Composite metric (adjust weights as needed)
    composite_metric = (0.4 * f1) + (0.3 * roc_auc) + (0.2 * mcc) + (0.1 * balanced_acc)

    return {
        "accuracy": accuracy,
        "f1": f1,
        "roc_auc": roc_auc,
        "mcc": mcc,
        "balanced_accuracy": balanced_acc,
        "composite_metric": composite_metric,
    }

# -----------------------------
# Early Stopping Callback
# -----------------------------
callbacks = [
    EarlyStoppingCallback(early_stopping_patience=10)
]

# -----------------------------
# Custom Trainer for Logging
# -----------------------------
class CustomTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super(CustomTrainer, self).__init__(*args, **kwargs)
        self.writer = SummaryWriter(log_dir="./tensorboard_logs")

    def on_step_end(self, args, state, control, **kwargs):
        # Log gradient norms
        total_norm = 0
        for p in self.model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** 0.5
        self.writer.add_scalar("Grad Norm", total_norm, state.global_step)

        # Log learning rate
        current_lr = self._get_learning_rate()
        self.writer.add_scalar("Learning Rate", current_lr, state.global_step)
        super().on_step_end(args, state, control, **kwargs)

    def _get_learning_rate(self):
        # Assuming one parameter group
        return self.optimizer.param_groups[0]['lr']

    def on_train_end(self, args, state, control, **kwargs):
        self.writer.close()
        super().on_train_end(args, state, control, **kwargs)

# -----------------------------
# Initialize and Train
# -----------------------------
trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset_hf,
    eval_dataset=val_dataset_hf,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
    callbacks=callbacks,
)

trainer.train()
trainer.save_model(output_dir)

# # -----------------------------
# # Example Inference
# # -----------------------------
# test_sentences = [
#     "The cat sat on the mat.",
#     "Running down the street"
# ]
# test_inputs = tokenizer(
#     test_sentences, 
#     return_tensors="pt", 
#     truncation=True, 
#     padding="max_length", 
#     max_length=128
# )

# model.eval()
# with torch.no_grad():
#     outputs = model(**test_inputs)
#     preds = torch.argmax(outputs.logits, dim=1).tolist()

# print("Predictions:", preds)  # 1 for complete, 0 for incomplete

# -----------------------------
# To view TensorBoard logs:
# tensorboard --logdir=./tensorboard_logs
# -----------------------------

```

## Contributing ğŸ¤

Got ideas or found a bug? Contributions are welcome! Feel free to open issues or submit pull requests.

## License ğŸ“œ

The core codebase of this project is released under the **MIT License** (see the [LICENSE](./LICENSE) file for details).

This project relies on external specific TTS engines (like `Coqui XTTSv2`) and LLM providers which have their **own licensing terms**. Please ensure you comply with the licenses of all components you use.
